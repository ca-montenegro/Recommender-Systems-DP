{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1zfgFLZ-Z5h8"
   },
   "source": [
    "# Eindhoven University of Technology, Netherlands.\n",
    "## Mathematics & Computer Science\n",
    "## Data Science Master track\n",
    "## Recommender Systems\n",
    "Lecturer: Dr. Vlado Menkovski\n",
    "- Franziska Boenisch\n",
    "- Adriano Cardace \n",
    "- Camilo Montenegro Hernandez\n",
    "\n",
    "## Sequence Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ItKSAoENZ5iK"
   },
   "source": [
    "## Task 1.2: Aspect-level Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ELfbtzylZ5iV"
   },
   "source": [
    "Build an attention-based aspect-level sentiment classification model with RNN. Your model shall\n",
    "include:\n",
    "\n",
    "- RNN network that learns sentence representation from input sequences.\n",
    "- Attention network that assigns attention score over a sequence of RNN hidden states based on aspect terms representation.\n",
    "- Fully connected network that predicts sentiment label, given the representation weighted by the attention score.\n",
    "\n",
    "Train the model by using data iterator and batch generator. Evaluate the trained model on\n",
    "the provided test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "13GNLSaWu7np"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yo7nUpwkZ5iy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import codecs\n",
    "import operator\n",
    "import numpy as np\n",
    "import re\n",
    "from time import time\n",
    "import _pickle as cPickle\n",
    "import IPython\n",
    "import keras\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "# doc_path = './doc_level-sentiment/doc_level'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 189
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 82380,
     "status": "ok",
     "timestamp": 1559569152070,
     "user": {
      "displayName": "Camilo Montenegro",
      "photoUrl": "https://lh6.googleusercontent.com/-Jn6HIGOBkEI/AAAAAAAAAAI/AAAAAAAAAGE/KRzSedPdx_c/s64/photo.jpg",
      "userId": "11354035389149471417"
     },
     "user_tz": -120
    },
    "id": "a0XuIy-XaFqg",
    "outputId": "5524538d-eb93-444d-f391-cd87266b6ad9"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# !cp './gdrive/My Drive/Colab Notebooks/2lMM10/assignment3/doc_level_sentiment/doc_level'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v7aioY8VaK3x"
   },
   "outputs": [],
   "source": [
    "aspect_path = 'gdrive/My Drive/aspect_level-sentiment/aspect_level'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d8eLp5-9Z5jW"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7w3B6WsneadN"
   },
   "source": [
    "Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h_tzR4rcZ5je"
   },
   "outputs": [],
   "source": [
    "num_regex = re.compile('^[+-]?[0-9]+\\.?[0-9]*$')\n",
    "\n",
    "def is_number(token):\n",
    "    return bool(num_regex.match(token))\n",
    "\n",
    "\n",
    "def create_vocab(domain, aspect_path, maxlen=0, vocab_size=0):\n",
    "    \n",
    "    assert domain in ['res_14', 'lt_14', 'res_15', 'res_16']\n",
    "\n",
    "    file_list = [os.path.join(aspect_path,'%s_train_sentence.txt'%(domain)),\n",
    "                 os.path.join(aspect_path,'%s_test_sentence.txt'%(domain))]\n",
    "\n",
    "    print ('Creating vocab ...')\n",
    "\n",
    "    total_words, unique_words = 0, 0\n",
    "    word_freqs = {}\n",
    "\n",
    "    for f in file_list:\n",
    "        top = 0\n",
    "        fin = codecs.open(f, 'r', 'utf-8')\n",
    "        for line in fin:\n",
    "            words = line.split()\n",
    "            if maxlen > 0 and len(words) > maxlen:\n",
    "                continue\n",
    "            for w in words:\n",
    "                if not is_number(w):\n",
    "                    try:\n",
    "                        word_freqs[w] += 1\n",
    "                    except KeyError:\n",
    "                        unique_words += 1\n",
    "                        word_freqs[w] = 1\n",
    "                    total_words += 1\n",
    "\n",
    "    print ('  %i total words, %i unique words' % (total_words, unique_words))\n",
    "    sorted_word_freqs = sorted(word_freqs.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "    vocab = {'<pad>':0, '<unk>':1, '<num>':2}\n",
    "    index = len(vocab)\n",
    "    for word, _ in sorted_word_freqs:\n",
    "        vocab[word] = index\n",
    "        index += 1\n",
    "        if vocab_size > 0 and index > vocab_size + 2:\n",
    "            break\n",
    "    if vocab_size > 0:\n",
    "        print (' keep the top %i words' % vocab_size)\n",
    "\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "06AB1xl1Z5kA"
   },
   "outputs": [],
   "source": [
    "def read_dataset_aspect(domain, aspect_path, phase, vocab, maxlen):\n",
    "    \n",
    "    assert domain in ['res_14', 'lt_14', 'res_15', 'res_16']\n",
    "    assert phase in ['train', 'test']\n",
    "    \n",
    "    print ('Preparing dataset ...')\n",
    "\n",
    "    data_x, data_y, aspect = [], [], []\n",
    "    polarity_category = {'positive': 0, 'negative': 1, 'neutral': 2}\n",
    "    \n",
    "    if(phase == 'train'):\n",
    "        file_names = [os.path.join(aspect_path,'%s_%s_sentence.txt'%(domain, phase)),\n",
    "                   os.path.join(aspect_path,'%s_%s_polarity.txt'%(domain, phase)),\n",
    "                   os.path.join(aspect_path,'%s_%s_term.txt'%(domain, phase))]\n",
    "    else:\n",
    "        file_names = [os.path.join(aspect_path, '%s_%s_sentence.txt'%(domain, phase)),\n",
    "                   os.path.join(aspect_path, '%s_%s_polarity.txt'%(domain, phase)),\n",
    "                   os.path.join(aspect_path, '%s_%s_term.txt'%(domain, phase))]\n",
    "\n",
    "    num_hit, unk_hit, total = 0., 0., 0.\n",
    "    maxlen_x = 0\n",
    "    maxlen_aspect = 0\n",
    "\n",
    "    files = [open(i, 'r') for i in file_names]\n",
    "    for rows in zip(*files):\n",
    "        content = rows[0].strip().split()\n",
    "        polarity = rows[1].strip()\n",
    "        aspect_content = rows[2].strip().split()\n",
    "\n",
    "        if maxlen > 0 and len(content) > maxlen:\n",
    "            continue\n",
    "\n",
    "        content_indices = []\n",
    "        if len(content) == 0:\n",
    "            content_indices.append(vocab['<unk>'])\n",
    "            unk_hit += 1\n",
    "        for word in content:\n",
    "            if is_number(word):\n",
    "                content_indices.append(vocab['<num>'])\n",
    "                num_hit += 1\n",
    "            elif word in vocab:\n",
    "                content_indices.append(vocab[word])\n",
    "            else:\n",
    "                content_indices.append(vocab['<unk>'])\n",
    "                unk_hit += 1\n",
    "            total += 1\n",
    "\n",
    "        data_x.append(content_indices)\n",
    "        data_y.append(polarity_category[polarity])\n",
    "\n",
    "        aspect_indices = []\n",
    "        if len(aspect_content) == 0:\n",
    "            aspect_indices.append(vocab['<unk>'])\n",
    "            unk_hit += 1\n",
    "        for word in aspect_content:\n",
    "            if is_number(word):\n",
    "                aspect_indices.append(vocab['<num>'])\n",
    "            elif word in vocab:\n",
    "                aspect_indices.append(vocab[word])\n",
    "            else:\n",
    "                aspect_indices.append(vocab['<unk>'])\n",
    "        aspect.append(aspect_indices)\n",
    "\n",
    "        if maxlen_x < len(content_indices):\n",
    "            maxlen_x = len(content_indices)\n",
    "        if maxlen_aspect < len(aspect_indices):\n",
    "            maxlen_aspect = len(aspect_indices)\n",
    "\n",
    "\n",
    "    \n",
    "    print ('  <num> hit rate: %.2f%%, <unk> hit rate: %.2f%%' % (100*num_hit/total, 100*unk_hit/total))\n",
    "    return data_x, data_y, aspect, maxlen_x, maxlen_aspect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "44lmpI-2Z5ke"
   },
   "outputs": [],
   "source": [
    "def get_data_aspect(vocab, domain, aspect_path, maxlen=0):\n",
    "    \n",
    "    assert domain in ['res_14', 'lt_14', 'res_15', 'res_16']\n",
    "\n",
    "    train_x, train_y, train_aspect, train_maxlen, train_maxlen_aspect = \\\n",
    "    read_dataset_aspect(domain, aspect_path, 'train', vocab, maxlen)\n",
    "    \n",
    "    test_x, test_y, test_aspect, test_maxlen, test_maxlen_aspect = \\\n",
    "    read_dataset_aspect(domain, aspect_path, 'test', vocab, maxlen)\n",
    "    \n",
    "    overal_maxlen = max(train_maxlen, test_maxlen)\n",
    "    overal_maxlen_aspect = max(train_maxlen_aspect, test_maxlen_aspect)\n",
    "\n",
    "    print (' Overal_maxlen: %s' % overal_maxlen)\n",
    "    print (' Overal_maxlen_aspect:%s '% overal_maxlen_aspect)\n",
    "    \n",
    "    return train_x, train_y, train_aspect, test_x, test_y, test_aspect, overal_maxlen, overal_maxlen_aspect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bt2NJSahZ5k6"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ejj-FqdNegnX"
   },
   "source": [
    "Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3691,
     "status": "ok",
     "timestamp": 1559569199071,
     "user": {
      "displayName": "Camilo Montenegro",
      "photoUrl": "https://lh6.googleusercontent.com/-Jn6HIGOBkEI/AAAAAAAAAAI/AAAAAAAAAGE/KRzSedPdx_c/s64/photo.jpg",
      "userId": "11354035389149471417"
     },
     "user_tz": -120
    },
    "id": "gYDNEY-3Z5lX",
    "outputId": "0eabb1d8-dd16-4654-f9c8-9a06f5d03d13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocab ...\n",
      "  57377 total words, 3253 unique words\n",
      " keep the top 10000 words\n",
      "Preparing dataset ...\n",
      "  <num> hit rate: 0.99%, <unk> hit rate: 0.03%\n",
      "Preparing dataset ...\n",
      "  <num> hit rate: 1.18%, <unk> hit rate: 0.07%\n",
      " Overal_maxlen: 82\n",
      " Overal_maxlen_aspect:7 \n"
     ]
    }
   ],
   "source": [
    "vocab = create_vocab('lt_14', aspect_path, 0, 10000)\n",
    "train_x, train_y, train_aspect, test_x, test_y, \\\n",
    "test_aspect, overal_maxlen, overal_maxlen_aspect = get_data_aspect(vocab, 'lt_14', aspect_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1014,
     "status": "ok",
     "timestamp": 1559569201227,
     "user": {
      "displayName": "Camilo Montenegro",
      "photoUrl": "https://lh6.googleusercontent.com/-Jn6HIGOBkEI/AAAAAAAAAAI/AAAAAAAAAGE/KRzSedPdx_c/s64/photo.jpg",
      "userId": "11354035389149471417"
     },
     "user_tz": -120
    },
    "id": "gqhVJjMKGxMZ",
    "outputId": "3e21fcc6-76be-44c4-fa33-37e6584b0663"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', ',', ',', ',', ',', ',', 'and', 'i', 'it', 'of', 'that', 'my', 'like', '!', 'more', 'one', 'even', 'because', 'features', 'got', 'offers', 'photobooth', 'band', 'garage', 'son', 'teenage', 'ichat']\n",
      "0\n",
      "['features']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "ans = []\n",
    "for i,item in vocab.items():\n",
    "  for word in train_x[10]:\n",
    "    if word==item:\n",
    "      ans.append(i)\n",
    "print(ans)\n",
    "print(train_y[10])\n",
    "\n",
    "ans = []\n",
    "for i,item in vocab.items():\n",
    "  for word in train_aspect[10]:\n",
    "    if word==item:\n",
    "      ans.append(i)\n",
    "print(ans)\n",
    "print(train_y[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1147,
     "status": "ok",
     "timestamp": 1559569203268,
     "user": {
      "displayName": "Camilo Montenegro",
      "photoUrl": "https://lh6.googleusercontent.com/-Jn6HIGOBkEI/AAAAAAAAAAI/AAAAAAAAAGE/KRzSedPdx_c/s64/photo.jpg",
      "userId": "11354035389149471417"
     },
     "user_tz": -120
    },
    "id": "45VRMYIvZ5mF",
    "outputId": "4edaa472-04d8-447b-ed63-9434f3ce115d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation size: 462\n"
     ]
    }
   ],
   "source": [
    "# Pad aspect sentences sequences for mini-batch processing\n",
    "train_x = sequence.pad_sequences(train_x, maxlen=overal_maxlen)\n",
    "test_x = sequence.pad_sequences(test_x, maxlen=overal_maxlen)\n",
    "train_aspect = sequence.pad_sequences(train_aspect, maxlen=overal_maxlen_aspect)\n",
    "test_aspect = sequence.pad_sequences(test_aspect, maxlen=overal_maxlen_aspect)\n",
    "\n",
    "# convert y to categorical labels\n",
    "train_y = to_categorical(train_y, 3)\n",
    "test_y = to_categorical(test_y, 3)\n",
    "\n",
    "validation_ratio = 0.2\n",
    "validation_size = int(len(train_x) * validation_ratio)\n",
    "print ('Validation size: %s' % validation_size)\n",
    "\n",
    "\n",
    "dev_x = train_x[:validation_size]\n",
    "dev_y = train_y[:validation_size]\n",
    "dev_aspect = train_aspect[:validation_size]\n",
    "\n",
    "train_x = train_x[validation_size:]\n",
    "train_y = train_y[validation_size:]\n",
    "train_aspect = train_aspect[validation_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7py0yIkGZ5ms"
   },
   "outputs": [],
   "source": [
    "def read_pickle(data_path, file_name):\n",
    "\n",
    "    f = open(os.path.join(data_path, file_name), 'rb')\n",
    "    read_file = cPickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    return read_file\n",
    "\n",
    "def save_pickle(data_path, file_name, data):\n",
    "\n",
    "    f = open(os.path.join(data_path, file_name), 'wb')\n",
    "    cPickle.dump(data, f)\n",
    "    print(\" file saved to: %s\"%(os.path.join(data_path, file_name)))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4486,
     "status": "ok",
     "timestamp": 1559569209202,
     "user": {
      "displayName": "Camilo Montenegro",
      "photoUrl": "https://lh6.googleusercontent.com/-Jn6HIGOBkEI/AAAAAAAAAAI/AAAAAAAAAGE/KRzSedPdx_c/s64/photo.jpg",
      "userId": "11354035389149471417"
     },
     "user_tz": -120
    },
    "id": "p5WRm2AyZ5nH",
    "outputId": "9c93db60-e9bd-435e-a993-87cf6b930cbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " file saved to: gdrive/My Drive/aspect_level-sentiment/aspect_level/all_vocab.pkl\n",
      " file saved to: gdrive/My Drive/aspect_level-sentiment/aspect_level/train_x.pkl\n",
      " file saved to: gdrive/My Drive/aspect_level-sentiment/aspect_level/train_y.pkl\n",
      " file saved to: gdrive/My Drive/aspect_level-sentiment/aspect_level/dev_x.pkl\n",
      " file saved to: gdrive/My Drive/aspect_level-sentiment/aspect_level/dev_y.pkl\n",
      " file saved to: gdrive/My Drive/aspect_level-sentiment/aspect_level/test_x.pkl\n",
      " file saved to: gdrive/My Drive/aspect_level-sentiment/aspect_level/test_y.pkl\n",
      " file saved to: gdrive/My Drive/aspect_level-sentiment/aspect_level/train_aspect.pkl\n",
      " file saved to: gdrive/My Drive/aspect_level-sentiment/aspect_level/dev_aspect.pkl\n",
      " file saved to: gdrive/My Drive/aspect_level-sentiment/aspect_level/test_aspect.pkl\n"
     ]
    }
   ],
   "source": [
    "save_pickle(aspect_path, 'all_vocab.pkl', vocab)\n",
    "\n",
    "save_pickle(aspect_path, 'train_x.pkl', train_x)\n",
    "save_pickle(aspect_path, 'train_y.pkl', train_y)\n",
    "save_pickle(aspect_path, 'dev_x.pkl', dev_x)\n",
    "save_pickle(aspect_path, 'dev_y.pkl', dev_y)\n",
    "save_pickle(aspect_path, 'test_x.pkl', test_x)\n",
    "save_pickle(aspect_path, 'test_y.pkl', test_y)\n",
    "\n",
    "save_pickle(aspect_path, 'train_aspect.pkl', train_aspect)\n",
    "save_pickle(aspect_path, 'dev_aspect.pkl', dev_aspect)\n",
    "save_pickle(aspect_path, 'test_aspect.pkl', test_aspect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o37X_io7Z5ng"
   },
   "outputs": [],
   "source": [
    "vocab = read_pickle(aspect_path, 'all_vocab.pkl')\n",
    "\n",
    "train_x = read_pickle(aspect_path, 'train_x.pkl')\n",
    "train_y = read_pickle(aspect_path, 'train_y.pkl')\n",
    "dev_x = read_pickle(aspect_path, 'dev_x.pkl')\n",
    "dev_y = read_pickle(aspect_path, 'dev_y.pkl')\n",
    "test_x = read_pickle(aspect_path, 'test_x.pkl')\n",
    "test_y = read_pickle(aspect_path, 'test_y.pkl')\n",
    "\n",
    "train_aspect = read_pickle(aspect_path, 'train_aspect.pkl')\n",
    "dev_aspect = read_pickle(aspect_path, 'dev_aspect.pkl')\n",
    "test_aspect = read_pickle(aspect_path, 'test_aspect.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8PauS0ShZ5n3"
   },
   "outputs": [],
   "source": [
    "class Dataiterator():\n",
    "    '''\n",
    "      1) Iteration over minibatches using next(); call reset() between epochs to randomly shuffle the data\n",
    "      2) Access to the entire dataset using all()\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, aspect_data, seq_length=32, decoder_dim=300, batch_size=32):\n",
    "        \n",
    "        len_aspect_data = len(aspect_data[0])\n",
    "        \n",
    "        self.X_aspect = aspect_data[0] \n",
    "        self.y_aspect = aspect_data[1]\n",
    "        self.aspect_terms = aspect_data[2]\n",
    "        \n",
    "        self.num_data = len_aspect_data\n",
    "        self.batch_size = batch_size # batch size\n",
    "        self.reset() # initial: shuffling examples and set index to 0\n",
    "    \n",
    "    def __iter__(self): # iterates data\n",
    "        return self\n",
    "\n",
    "\n",
    "    def reset(self): # initials\n",
    "        self.idx = 0\n",
    "        self.order = np.random.permutation(self.num_data) # shuffling examples by providing randomized ids \n",
    "        \n",
    "    def __next__(self): # return model inputs - outputs per batch\n",
    "        \n",
    "        X_ids = [] # hold ids per batch \n",
    "        while len(X_ids) < self.batch_size:\n",
    "            X_id = self.order[self.idx] # copy random id from initial shuffling\n",
    "            X_ids.append(X_id)\n",
    "            self.idx += 1 # \n",
    "            if self.idx >= self.num_data: # exception if all examples of data have been seen (iterated)\n",
    "                self.reset()\n",
    "                raise StopIteration()\n",
    "                \n",
    "        batch_X_aspect = self.X_aspect[np.array(X_ids)] # X values (encoder input) per batch\n",
    "        batch_y_aspect = self.y_aspect[np.array(X_ids)] # y_in values (decoder input) per batch\n",
    "        batch_aspect_terms = self.aspect_terms[np.array(X_ids)]\n",
    "        \n",
    "        return batch_X_aspect, batch_y_aspect, batch_aspect_terms\n",
    "\n",
    "          \n",
    "    def all(self): # return all data examples\n",
    "        return self.X_aspect, self.y_aspect, self.aspect_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6koR1FM9eoD_"
   },
   "source": [
    "Construct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FwKLeIdGZ5oO"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dense, Lambda, Dropout, LSTM\n",
    "from keras.layers import Reshape, Activation, RepeatVector, concatenate, Concatenate, Dot, Multiply\n",
    "import keras.backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "euUMXHHlZ5of"
   },
   "outputs": [],
   "source": [
    "def custom_softmax(x, axis=1):\n",
    "            \"\"\"Softmax activation function.\n",
    "            # Arguments\n",
    "                x : Tensor.\n",
    "                axis: Integer, axis along which the softmax normalization is applied.\n",
    "            # Returns\n",
    "                Tensor, output of softmax transformation.\n",
    "            # Raises\n",
    "                ValueError: In case `dim(x) == 1`.\n",
    "            \"\"\"\n",
    "            ndim = K.ndim(x)\n",
    "            if ndim == 2:\n",
    "                return K.softmax(x)\n",
    "            elif ndim > 2:\n",
    "                e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "                s = K.sum(e, axis=axis, keepdims=True)\n",
    "                return e / s\n",
    "            else:\n",
    "                raise ValueError('Cannot apply softmax to a tensor that is 1D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o6_9eQ1UZ5oy"
   },
   "outputs": [],
   "source": [
    "repeator = RepeatVector(overal_maxlen, name='repeator_att')\n",
    "concatenator = Concatenate(axis=-1, name='concator_att')\n",
    "densor1 = Dense(100, activation = \"tanh\", name='densor1_att')\n",
    "densor2 = Dense(1, activation = \"relu\", name='densor2_att')\n",
    "activator = Activation(custom_softmax, name='attention_weights')\n",
    "dotor = Dot(axes = 1, name='dotor_att')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ckYEw0TZ5pG"
   },
   "outputs": [],
   "source": [
    "def attention(keys, query):\n",
    "    dropout = 0.5\n",
    "    query = repeator(query)\n",
    "    print(\"query shape: %s\" %str(query._keras_shape))\n",
    "    concat = concatenator([keys, query])\n",
    "    print(\"concat shape: %s\" %str(concat._keras_shape))\n",
    "    e1 = densor1(concat)\n",
    "    print(\"e1 shape: %s\" %str(e1._keras_shape))\n",
    "    dropou = Dropout(dropout)\n",
    "    e1 = dropou(e1)\n",
    "    e2 = densor2(e1)\n",
    "    print(\"e2 shape: %s\" %str(e2._keras_shape))\n",
    "    alphas = activator(e2)\n",
    "    print(\"alphas shape: %s\" %str(alphas._keras_shape))\n",
    "    context = dotor([alphas, keys])\n",
    "    print(\"context shape: %s\" %str(context._keras_shape))\n",
    "    \n",
    "    return context, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bqpwxmcrZ5pb"
   },
   "outputs": [],
   "source": [
    "class Average(Layer):\n",
    "  \n",
    "    def __init__(self, mask_zero=True, **kwargs):\n",
    "        self.mask_zero = mask_zero\n",
    "        self.supports_masking = True\n",
    "        super(Average, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        if self.mask_zero:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            tf.print(mask)\n",
    "            mask = K.expand_dims(mask)\n",
    "            x = x * mask\n",
    "            return K.sum(x, axis=1) / (K.sum(mask, axis=1) + K.epsilon())\n",
    "        else:\n",
    "            return K.mean(x, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "    \n",
    "    def compute_mask(self, x, mask):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1012
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1804,
     "status": "ok",
     "timestamp": 1559570570651,
     "user": {
      "displayName": "Camilo Montenegro",
      "photoUrl": "https://lh6.googleusercontent.com/-Jn6HIGOBkEI/AAAAAAAAAAI/AAAAAAAAAGE/KRzSedPdx_c/s64/photo.jpg",
      "userId": "11354035389149471417"
     },
     "user_tz": -120
    },
    "id": "SdVf_QOXZ5p1",
    "outputId": "8bc751de-e930-4968-95e7-0c598c87cf68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use average term embs as aspect embedding\n",
      "(?, 7, 100)\n",
      "(?, 100)\n",
      "query shape: (None, 82, 100)\n",
      "concat shape: (None, 82, 200)\n",
      "e1 shape: (None, 82, 100)\n",
      "e2 shape: (None, 82, 1)\n",
      "alphas shape: (None, 82, 1)\n",
      "context shape: (None, 1, 100)\n",
      "sentence_output shape: (None, 1, 3)\n",
      "sentence_output shape: (None, 3)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sentence_input (InputLayer)     (None, 82)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "aspect_input (InputLayer)       (None, 7)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_emb (Embedding)            multiple             325600      aspect_input[0][0]               \n",
      "                                                                 sentence_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 82, 100)      80400       word_emb[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "aspect_emb (Average)            (None, 100)          0           word_emb[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 82, 100)      0           lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "repeator_att (RepeatVector)     (None, 82, 100)      0           aspect_emb[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concator_att (Concatenate)      (None, 82, 200)      0           dropout_25[0][0]                 \n",
      "                                                                 repeator_att[6][0]               \n",
      "__________________________________________________________________________________________________\n",
      "densor1_att (Dense)             (None, 82, 100)      20100       concator_att[6][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 82, 100)      0           densor1_att[6][0]                \n",
      "__________________________________________________________________________________________________\n",
      "densor2_att (Dense)             (None, 82, 1)        101         dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 82, 1)        0           densor2_att[6][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dotor_att (Dot)                 (None, 1, 100)       0           attention_weights[6][0]          \n",
      "                                                                 dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 1, 100)       0           dotor_att[6][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 3)         303         dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 3)            0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "aspect_model (Activation)       (None, 3)            0           reshape_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 426,504\n",
      "Trainable params: 426,504\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dropout = 0.6\n",
    "recurrent_dropout = 0.5\n",
    "vocab_size = len(vocab)\n",
    "num_outputs = 3 # labels\n",
    "\n",
    "##### Inputs #####\n",
    "sentence_input = Input(shape=(overal_maxlen,), dtype='int32', name='sentence_input')\n",
    "aspect_input = Input(shape=(overal_maxlen_aspect,), dtype='int32', name='aspect_input')\n",
    "\n",
    "word_emb = Embedding(vocab_size, 100, mask_zero=True, name='word_emb')\n",
    "\n",
    "### represent aspect as averaged word embedding ###\n",
    "print ('use average term embs as aspect embedding')\n",
    "aspect_term_embs = word_emb(aspect_input)\n",
    "print(aspect_term_embs.shape)\n",
    "aspect_embs = Average(mask_zero=True, name='aspect_emb')(aspect_term_embs)\n",
    "print(aspect_embs.shape)\n",
    "sentence_embs = word_emb(sentence_input) # from aspect-level domain\n",
    "rnn = LSTM(100, return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout, name='lstm')\n",
    "sentence_lstm = rnn(sentence_embs)\n",
    "drop = Dropout(dropout)\n",
    "sentence_lstm = drop(sentence_lstm)\n",
    "att_context, att_weights = attention(sentence_lstm, aspect_embs)\n",
    "drop1 = Dropout(dropout)\n",
    "att_context = drop1(att_context)\n",
    "sentence_output = Dense(num_outputs, name='dense_1')(att_context)\n",
    "#dropoutLayer = Dropout(dropout)\n",
    "#sentence_output = dropoutLayer(sentence_output)\n",
    "print(\"sentence_output shape: %s\" % str(sentence_output._keras_shape))\n",
    "sentence_output = Reshape((num_outputs,))(sentence_output)\n",
    "print(\"sentence_output shape: %s\" % str(sentence_output._keras_shape))\n",
    "\n",
    "aspect_probs = Activation('softmax', name='aspect_model')(sentence_output)\n",
    "\n",
    "model = Model(inputs=[sentence_input, aspect_input], outputs=[aspect_probs])\n",
    "\n",
    "import keras.optimizers as opt\n",
    "\n",
    "optimizer = opt.Adam(lr=0.001)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss={'aspect_model': 'categorical_crossentropy'},\n",
    "              metrics = {'aspect_model': 'categorical_accuracy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1346
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 970,
     "status": "ok",
     "timestamp": 1559570573264,
     "user": {
      "displayName": "Camilo Montenegro",
      "photoUrl": "https://lh6.googleusercontent.com/-Jn6HIGOBkEI/AAAAAAAAAAI/AAAAAAAAAGE/KRzSedPdx_c/s64/photo.jpg",
      "userId": "11354035389149471417"
     },
     "user_tz": -120
    },
    "id": "3WILdTQQc7Jm",
    "outputId": "0733a36b-aea7-4db5-ef47-abfcb08cdd7f"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"994pt\" viewBox=\"0.00 0.00 359.50 994.00\" width=\"360pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 990)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-990 355.5,-990 355.5,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140493929932952 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140493929932952</title>\n",
       "<polygon fill=\"none\" points=\"0,-949.5 0,-985.5 173,-985.5 173,-949.5 0,-949.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"86.5\" y=\"-963.8\">sentence_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140493929933008 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140493929933008</title>\n",
       "<polygon fill=\"none\" points=\"101.5,-876.5 101.5,-912.5 255.5,-912.5 255.5,-876.5 101.5,-876.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"178.5\" y=\"-890.8\">word_emb: Embedding</text>\n",
       "</g>\n",
       "<!-- 140493929932952&#45;&gt;140493929933008 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140493929932952-&gt;140493929933008</title>\n",
       "<path d=\"M109.2416,-949.4551C120.8523,-940.2422 135.1458,-928.9006 147.7266,-918.918\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"150.0431,-921.5479 155.7012,-912.5904 145.6921,-916.0644 150.0431,-921.5479\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140493929933064 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140493929933064</title>\n",
       "<polygon fill=\"none\" points=\"191.5,-949.5 191.5,-985.5 351.5,-985.5 351.5,-949.5 191.5,-949.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"271.5\" y=\"-963.8\">aspect_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140493929933064&#45;&gt;140493929933008 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140493929933064-&gt;140493929933008</title>\n",
       "<path d=\"M248.5112,-949.4551C236.7743,-940.2422 222.3254,-928.9006 209.6079,-918.918\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"211.5738,-916.0117 201.5466,-912.5904 207.2517,-921.518 211.5738,-916.0117\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140493929933512 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140493929933512</title>\n",
       "<polygon fill=\"none\" points=\"70,-803.5 70,-839.5 157,-839.5 157,-803.5 70,-803.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"113.5\" y=\"-817.8\">lstm: LSTM</text>\n",
       "</g>\n",
       "<!-- 140493929933008&#45;&gt;140493929933512 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140493929933008-&gt;140493929933512</title>\n",
       "<path d=\"M162.4326,-876.4551C154.5419,-867.5932 144.8974,-856.7616 136.263,-847.0646\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"138.8718,-844.7313 129.6079,-839.5904 133.6439,-849.3863 138.8718,-844.7313\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140493929480544 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140493929480544</title>\n",
       "<polygon fill=\"none\" points=\"185,-803.5 185,-839.5 322,-839.5 322,-803.5 185,-803.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"253.5\" y=\"-817.8\">aspect_emb: Average</text>\n",
       "</g>\n",
       "<!-- 140493929933008&#45;&gt;140493929480544 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>140493929933008-&gt;140493929480544</title>\n",
       "<path d=\"M197.0393,-876.4551C206.2342,-867.5054 217.4928,-856.547 227.5305,-846.7769\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"230.1892,-849.0734 234.914,-839.5904 225.3068,-844.0572 230.1892,-849.0734\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140493929700936 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>140493929700936</title>\n",
       "<polygon fill=\"none\" points=\"22,-730.5 22,-766.5 163,-766.5 163,-730.5 22,-730.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"92.5\" y=\"-744.8\">dropout_25: Dropout</text>\n",
       "</g>\n",
       "<!-- 140493929933512&#45;&gt;140493929700936 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>140493929933512-&gt;140493929700936</title>\n",
       "<path d=\"M108.309,-803.4551C105.9616,-795.2951 103.1338,-785.4652 100.5228,-776.3887\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"103.8323,-775.233 97.7041,-766.5904 97.1052,-777.1683 103.8323,-775.233\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140495695605376 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>140495695605376</title>\n",
       "<polygon fill=\"none\" points=\"181,-730.5 181,-766.5 348,-766.5 348,-730.5 181,-730.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"264.5\" y=\"-744.8\">repeator_att: RepeatVector</text>\n",
       "</g>\n",
       "<!-- 140493929480544&#45;&gt;140495695605376 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>140493929480544-&gt;140495695605376</title>\n",
       "<path d=\"M256.2191,-803.4551C257.4355,-795.3828 258.8981,-785.6764 260.2534,-776.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"263.7449,-777.0003 261.7741,-766.5904 256.823,-775.9572 263.7449,-777.0003\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140495686053280 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>140495686053280</title>\n",
       "<polygon fill=\"none\" points=\"130.5,-657.5 130.5,-693.5 294.5,-693.5 294.5,-657.5 130.5,-657.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"212.5\" y=\"-671.8\">concator_att: Concatenate</text>\n",
       "</g>\n",
       "<!-- 140493929700936&#45;&gt;140495686053280 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>140493929700936-&gt;140495686053280</title>\n",
       "<path d=\"M122.1629,-730.4551C137.74,-720.979 157.0193,-709.2508 173.7617,-699.0658\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"176.0381,-701.7778 182.7624,-693.5904 172.4,-695.7974 176.0381,-701.7778\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140494506291384 -->\n",
       "<g class=\"node\" id=\"node13\">\n",
       "<title>140494506291384</title>\n",
       "<polygon fill=\"none\" points=\"103.5,-292.5 103.5,-328.5 201.5,-328.5 201.5,-292.5 103.5,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"152.5\" y=\"-306.8\">dotor_att: Dot</text>\n",
       "</g>\n",
       "<!-- 140493929700936&#45;&gt;140494506291384 -->\n",
       "<g class=\"edge\" id=\"edge14\">\n",
       "<title>140493929700936-&gt;140494506291384</title>\n",
       "<path d=\"M92.5,-730.062C92.5,-702.3663 92.5,-648.3785 92.5,-602.5 92.5,-602.5 92.5,-602.5 92.5,-456.5 92.5,-415.0539 93.3954,-402.752 110.5,-365 115.0578,-354.9404 121.8014,-345.0385 128.5576,-336.4925\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"131.2844,-338.6873 134.9681,-328.7535 125.8936,-334.2219 131.2844,-338.6873\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140495695605376&#45;&gt;140495686053280 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>140495695605376-&gt;140495686053280</title>\n",
       "<path d=\"M251.6461,-730.4551C245.521,-721.8564 238.075,-711.4034 231.328,-701.9316\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"234.0389,-699.7046 225.3863,-693.5904 228.3374,-703.7659 234.0389,-699.7046\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140495679136432 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>140495679136432</title>\n",
       "<polygon fill=\"none\" points=\"149.5,-584.5 149.5,-620.5 275.5,-620.5 275.5,-584.5 149.5,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"212.5\" y=\"-598.8\">densor1_att: Dense</text>\n",
       "</g>\n",
       "<!-- 140495686053280&#45;&gt;140495679136432 -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>140495686053280-&gt;140495679136432</title>\n",
       "<path d=\"M212.5,-657.4551C212.5,-649.3828 212.5,-639.6764 212.5,-630.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"216.0001,-630.5903 212.5,-620.5904 209.0001,-630.5904 216.0001,-630.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140493938555200 -->\n",
       "<g class=\"node\" id=\"node10\">\n",
       "<title>140493938555200</title>\n",
       "<polygon fill=\"none\" points=\"142,-511.5 142,-547.5 283,-547.5 283,-511.5 142,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"212.5\" y=\"-525.8\">dropout_26: Dropout</text>\n",
       "</g>\n",
       "<!-- 140495679136432&#45;&gt;140493938555200 -->\n",
       "<g class=\"edge\" id=\"edge10\">\n",
       "<title>140495679136432-&gt;140493938555200</title>\n",
       "<path d=\"M212.5,-584.4551C212.5,-576.3828 212.5,-566.6764 212.5,-557.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"216.0001,-557.5903 212.5,-547.5904 209.0001,-557.5904 216.0001,-557.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140495679138784 -->\n",
       "<g class=\"node\" id=\"node11\">\n",
       "<title>140495679138784</title>\n",
       "<polygon fill=\"none\" points=\"149.5,-438.5 149.5,-474.5 275.5,-474.5 275.5,-438.5 149.5,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"212.5\" y=\"-452.8\">densor2_att: Dense</text>\n",
       "</g>\n",
       "<!-- 140493938555200&#45;&gt;140495679138784 -->\n",
       "<g class=\"edge\" id=\"edge11\">\n",
       "<title>140493938555200-&gt;140495679138784</title>\n",
       "<path d=\"M212.5,-511.4551C212.5,-503.3828 212.5,-493.6764 212.5,-484.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"216.0001,-484.5903 212.5,-474.5904 209.0001,-484.5904 216.0001,-484.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140494506233696 -->\n",
       "<g class=\"node\" id=\"node12\">\n",
       "<title>140494506233696</title>\n",
       "<polygon fill=\"none\" points=\"120,-365.5 120,-401.5 305,-401.5 305,-365.5 120,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"212.5\" y=\"-379.8\">attention_weights: Activation</text>\n",
       "</g>\n",
       "<!-- 140495679138784&#45;&gt;140494506233696 -->\n",
       "<g class=\"edge\" id=\"edge12\">\n",
       "<title>140495679138784-&gt;140494506233696</title>\n",
       "<path d=\"M212.5,-438.4551C212.5,-430.3828 212.5,-420.6764 212.5,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"216.0001,-411.5903 212.5,-401.5904 209.0001,-411.5904 216.0001,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140494506233696&#45;&gt;140494506291384 -->\n",
       "<g class=\"edge\" id=\"edge13\">\n",
       "<title>140494506233696-&gt;140494506291384</title>\n",
       "<path d=\"M197.6685,-365.4551C190.4569,-356.6809 181.6583,-345.9759 173.749,-336.353\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"176.4224,-334.0934 167.3688,-328.5904 171.0146,-338.5382 176.4224,-334.0934\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140493941679216 -->\n",
       "<g class=\"node\" id=\"node14\">\n",
       "<title>140493941679216</title>\n",
       "<polygon fill=\"none\" points=\"82,-219.5 82,-255.5 223,-255.5 223,-219.5 82,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"152.5\" y=\"-233.8\">dropout_27: Dropout</text>\n",
       "</g>\n",
       "<!-- 140494506291384&#45;&gt;140493941679216 -->\n",
       "<g class=\"edge\" id=\"edge15\">\n",
       "<title>140494506291384-&gt;140493941679216</title>\n",
       "<path d=\"M152.5,-292.4551C152.5,-284.3828 152.5,-274.6764 152.5,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"156.0001,-265.5903 152.5,-255.5904 149.0001,-265.5904 156.0001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140493929933344 -->\n",
       "<g class=\"node\" id=\"node15\">\n",
       "<title>140493929933344</title>\n",
       "<polygon fill=\"none\" points=\"99,-146.5 99,-182.5 206,-182.5 206,-146.5 99,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"152.5\" y=\"-160.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 140493941679216&#45;&gt;140493929933344 -->\n",
       "<g class=\"edge\" id=\"edge16\">\n",
       "<title>140493941679216-&gt;140493929933344</title>\n",
       "<path d=\"M152.5,-219.4551C152.5,-211.3828 152.5,-201.6764 152.5,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"156.0001,-192.5903 152.5,-182.5904 149.0001,-192.5904 156.0001,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140493936560840 -->\n",
       "<g class=\"node\" id=\"node16\">\n",
       "<title>140493936560840</title>\n",
       "<polygon fill=\"none\" points=\"87.5,-73.5 87.5,-109.5 217.5,-109.5 217.5,-73.5 87.5,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"152.5\" y=\"-87.8\">reshape_6: Reshape</text>\n",
       "</g>\n",
       "<!-- 140493929933344&#45;&gt;140493936560840 -->\n",
       "<g class=\"edge\" id=\"edge17\">\n",
       "<title>140493929933344-&gt;140493936560840</title>\n",
       "<path d=\"M152.5,-146.4551C152.5,-138.3828 152.5,-128.6764 152.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"156.0001,-119.5903 152.5,-109.5904 149.0001,-119.5904 156.0001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140493936561400 -->\n",
       "<g class=\"node\" id=\"node17\">\n",
       "<title>140493936561400</title>\n",
       "<polygon fill=\"none\" points=\"72,-.5 72,-36.5 233,-36.5 233,-.5 72,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"152.5\" y=\"-14.8\">aspect_model: Activation</text>\n",
       "</g>\n",
       "<!-- 140493936560840&#45;&gt;140493936561400 -->\n",
       "<g class=\"edge\" id=\"edge18\">\n",
       "<title>140493936560840-&gt;140493936561400</title>\n",
       "<path d=\"M152.5,-73.4551C152.5,-65.3828 152.5,-55.6764 152.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"156.0001,-46.5903 152.5,-36.5904 149.0001,-46.5904 156.0001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPython.display.SVG(keras.utils.vis_utils.model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jiiw9AaEZ5qT"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_steps_epoch = len(train_x)/batch_size\n",
    "# print(train_y.shape)\n",
    "# print(dev_y.shape)\n",
    "\n",
    "batch_train_iter = Dataiterator([train_x, train_y, train_aspect], batch_size)\n",
    "val_steps_epoch = len(dev_x)/batch_size\n",
    "batch_val_iter = Dataiterator([dev_x, dev_y, dev_aspect], batch_size)\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def train_generator(model, batch_train_iter, batch_val_iter):\n",
    "    \n",
    "    earlystop_callbacks = [EarlyStopping(monitor='val_loss', patience=10),\n",
    "                     ModelCheckpoint(filepath=os.path.join('./','{epoch:02d}-{loss:.2f}.check'), \\\n",
    "                                     monitor='val_loss', save_best_only=False, \\\n",
    "                                     save_weights_only=True)\n",
    "                     ]\n",
    "    \n",
    "    def train_gen():\n",
    "        while True:\n",
    "            train_batches = [[[X, aspect], [y]] for X, y, aspect in batch_train_iter]\n",
    "            for train_batch in train_batches:\n",
    "                yield train_batch\n",
    "                \n",
    "                \n",
    "    def val_gen():\n",
    "        while True:\n",
    "            val_batches = [[[X, aspect], [y]] for X, y, aspect in batch_val_iter]\n",
    "            for val_batch in val_batches:\n",
    "                yield val_batch\n",
    "                \n",
    "    history = model.fit_generator(train_gen(), validation_data=val_gen(), \\\n",
    "                                  validation_steps=val_steps_epoch, steps_per_epoch=train_steps_epoch, \\\n",
    "                                  epochs = 20, callbacks = earlystop_callbacks)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "usWiCiNffWaJ"
   },
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1020
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 253844,
     "status": "ok",
     "timestamp": 1559570851475,
     "user": {
      "displayName": "Camilo Montenegro",
      "photoUrl": "https://lh6.googleusercontent.com/-Jn6HIGOBkEI/AAAAAAAAAAI/AAAAAAAAAGE/KRzSedPdx_c/s64/photo.jpg",
      "userId": "11354035389149471417"
     },
     "user_tz": -120
    },
    "id": "Pfr9V0tpZ5qh",
    "outputId": "9a340a40-63c1-45a3-d097-15c42fda2f34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "58/57 [==============================] - 19s 334ms/step - loss: 1.0890 - categorical_accuracy: 0.4359 - val_loss: 1.0709 - val_categorical_accuracy: 0.4271\n",
      "Epoch 2/20\n",
      "58/57 [==============================] - 17s 285ms/step - loss: 0.9907 - categorical_accuracy: 0.5377 - val_loss: 0.8941 - val_categorical_accuracy: 0.6125\n",
      "Epoch 3/20\n",
      "58/57 [==============================] - 17s 285ms/step - loss: 0.8033 - categorical_accuracy: 0.6708 - val_loss: 0.8253 - val_categorical_accuracy: 0.6813\n",
      "Epoch 4/20\n",
      "58/57 [==============================] - 16s 283ms/step - loss: 0.7036 - categorical_accuracy: 0.7247 - val_loss: 0.8472 - val_categorical_accuracy: 0.6583\n",
      "Epoch 5/20\n",
      "58/57 [==============================] - 17s 287ms/step - loss: 0.6220 - categorical_accuracy: 0.7662 - val_loss: 0.8189 - val_categorical_accuracy: 0.6292\n",
      "Epoch 6/20\n",
      "58/57 [==============================] - 17s 285ms/step - loss: 0.5364 - categorical_accuracy: 0.8087 - val_loss: 0.9009 - val_categorical_accuracy: 0.5917\n",
      "Epoch 7/20\n",
      "58/57 [==============================] - 17s 285ms/step - loss: 0.5009 - categorical_accuracy: 0.8308 - val_loss: 0.9009 - val_categorical_accuracy: 0.5646\n",
      "Epoch 8/20\n",
      "58/57 [==============================] - 16s 284ms/step - loss: 0.4502 - categorical_accuracy: 0.8438 - val_loss: 0.9267 - val_categorical_accuracy: 0.6125\n",
      "Epoch 9/20\n",
      "58/57 [==============================] - 16s 282ms/step - loss: 0.4172 - categorical_accuracy: 0.8551 - val_loss: 0.9083 - val_categorical_accuracy: 0.6292\n",
      "Epoch 10/20\n",
      "58/57 [==============================] - 17s 285ms/step - loss: 0.4224 - categorical_accuracy: 0.8497 - val_loss: 0.9618 - val_categorical_accuracy: 0.6479\n",
      "Epoch 11/20\n",
      "58/57 [==============================] - 16s 284ms/step - loss: 0.3839 - categorical_accuracy: 0.8669 - val_loss: 1.0502 - val_categorical_accuracy: 0.5813\n",
      "Epoch 12/20\n",
      "58/57 [==============================] - 16s 282ms/step - loss: 0.3650 - categorical_accuracy: 0.8728 - val_loss: 1.0453 - val_categorical_accuracy: 0.6083\n",
      "Epoch 13/20\n",
      "58/57 [==============================] - 16s 283ms/step - loss: 0.3574 - categorical_accuracy: 0.8658 - val_loss: 1.0486 - val_categorical_accuracy: 0.6146\n",
      "Epoch 14/20\n",
      "58/57 [==============================] - 16s 284ms/step - loss: 0.3321 - categorical_accuracy: 0.8825 - val_loss: 1.1631 - val_categorical_accuracy: 0.6167\n",
      "Epoch 15/20\n",
      "58/57 [==============================] - 16s 282ms/step - loss: 0.3278 - categorical_accuracy: 0.8831 - val_loss: 1.0704 - val_categorical_accuracy: 0.6021\n"
     ]
    }
   ],
   "source": [
    "train_generator(model, batch_train_iter, batch_val_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "phkVOd8kZ5qx"
   },
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vdNBdjsMZ5q2"
   },
   "outputs": [],
   "source": [
    "testPhrases = [\"it s fast light and simple to use\",\"other than not being a fan of click pads industry standard these days and the lousy internal speakers it is hard for me to find things about this notebook i do not like especially considering the price tag\",\"tech support would not fix the problem unless i bought your plan for more money\"]\n",
    "testPhrases_array = []\n",
    "for phrase in testPhrases:\n",
    "  testPhrase_array = []\n",
    "  for word in phrase.split(' '):\n",
    "    idxWord = vocab[word]\n",
    "    testPhrase_array.append(idxWord)\n",
    "  testPhrases_array.append(testPhrase_array)\n",
    "\n",
    "aspects_levels = ['use','pads','support']\n",
    "aspects_levels_array = []\n",
    "for asp in aspects_levels:\n",
    "  aspects_levels_array1 = []\n",
    "  idxWord = vocab[asp]\n",
    "  aspects_levels_array1.append(idxWord)\n",
    "  aspects_levels_array.append(aspects_levels_array1)\n",
    "\n",
    "  \n",
    "  \n",
    "train_x_1 = sequence.pad_sequences(testPhrases_array, maxlen=overal_maxlen)\n",
    "train_aspect_1 = sequence.pad_sequences(aspects_levels_array, maxlen=overal_maxlen_aspect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xo_NIjAiJbof"
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for i in range(len(testPhrases_array)):\n",
    "  prediction = model.predict([np.expand_dims(train_x_1[i],0),np.expand_dims(train_aspect_1[i],0)])  \n",
    "  predictions.append(np.argmax(prediction.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1368,
     "status": "ok",
     "timestamp": 1559572376811,
     "user": {
      "displayName": "Camilo Montenegro",
      "photoUrl": "https://lh6.googleusercontent.com/-Jn6HIGOBkEI/AAAAAAAAAAI/AAAAAAAAAGE/KRzSedPdx_c/s64/photo.jpg",
      "userId": "11354035389149471417"
     },
     "user_tz": -120
    },
    "id": "yJw0ngjxMGs_",
    "outputId": "e5444cdd-839b-47a7-eb16-eb34d271cda1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['use', 'pads', 'support']\n"
     ]
    }
   ],
   "source": [
    "predictions = pd.DataFrame(predictions)\n",
    "testPhrases = pd.DataFrame(testPhrases)\n",
    "print(aspects_levels)\n",
    "aspects_levels_2 = pd.DataFrame(aspects_levels)\n",
    "testPhrases['Aspects'] = (aspects_levels_2)\n",
    "testPhrases['Predicted Label'] = (predictions)\n",
    "testPhrases.columns = ['Test review','Aspect','Predicted Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 874,
     "status": "ok",
     "timestamp": 1559572378992,
     "user": {
      "displayName": "Camilo Montenegro",
      "photoUrl": "https://lh6.googleusercontent.com/-Jn6HIGOBkEI/AAAAAAAAAAI/AAAAAAAAAGE/KRzSedPdx_c/s64/photo.jpg",
      "userId": "11354035389149471417"
     },
     "user_tz": -120
    },
    "id": "qjAWxn4uMJE4",
    "outputId": "8220bc0b-1792-4937-f38e-622461351de5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence evaluation:\n",
      "0: Positive. 1: Negative. 2: Neutral\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test review</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Predicted Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it s fast light and simple to use</td>\n",
       "      <td>use</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>other than not being a fan of click pads industry standard these days and the lousy internal speakers it is hard for me to find things about this notebook i do not like especially considering the price tag</td>\n",
       "      <td>pads</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tech support would not fix the problem unless i bought your plan for more money</td>\n",
       "      <td>support</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                     Test review  ... Predicted Label\n",
       "0  it s fast light and simple to use                                                                                                                                                                              ...  0             \n",
       "1  other than not being a fan of click pads industry standard these days and the lousy internal speakers it is hard for me to find things about this notebook i do not like especially considering the price tag  ...  2             \n",
       "2  tech support would not fix the problem unless i bought your plan for more money                                                                                                                                ...  1             \n",
       "\n",
       "[3 rows x 3 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Sentence evaluation:\")\n",
    "print(\"0: Positive. 1: Negative. 2: Neutral\")\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "display(testPhrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6EYUw7xtwny5"
   },
   "source": [
    "During training phase is possible to see that only 15 out of 20 epochs were performed by the model. This is due to the early stopping function. We also obtain a high difference between the train accuracy and the validation accuracy due to overfitting. After training several models and configurations (e.g. Learning rate, optimizers and layers) the best validation accuracy obtained was 68%."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment-3.1.2.Aspect-Level-Sentiment.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
