{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6BYbelXTYDHU"
   },
   "source": [
    "# Eindhoven University of Technology, Netherlands.\n",
    "## Mathematics & Computer Science\n",
    "## Data Science Master track\n",
    "## Recommender Systems\n",
    "Lecturer: Dr. Vlado Menkovski\n",
    "- Franziska Boenisch\n",
    "- Adriano Cardace \n",
    "- Camilo Montenegro Hernandez\n",
    "\n",
    "## Sequence Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EWtKJJzfYDHb"
   },
   "source": [
    "## Task 1.3: Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uMdDhfDGYDHg"
   },
   "source": [
    "1. What is the motivation of incorporating an \"attention mechanism\" in a Machine Translation task? What is the main issue that this attention trying to solve? Mention the advantage(s) as compared to the model without attention.\n",
    "\n",
    "\n",
    "2. Likewise, what is the motivation of adding an \"attention\" network in aspect-level sentiment classification? What is the main issue that this attention trying to solve? Mention the advantage(s) as compared to the model without attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MzgMH5vPYDHk"
   },
   "source": [
    "### YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-VVH2v8dYDHr"
   },
   "source": [
    "1. During Machine Translation task is necessary that the model focuses in subparts of the sentences rather than the full sentence at one step, for that purpose is why the attention mechanism must be built in the network. This done in order to mimic the human behaviour during the same task. Attention mechanisms in neural networks serve to orient perception as well as memory access. A NN with this mechanism will know how to disregard the noise and focus on what is relevant. The main advantage compared with a NN without attention is the accuracy performance, the attention mechanism is capable of actually understand what \"it\" is referring to in a sentence, so in similar situations, this will increase the accuracy.\n",
    "\n",
    "2. In the attention-based LSTM networks for aspect-level sentiment classification the LSTM aims to capture sequential patterns and the attention mechanism aims to emphasize important words for the semantic classification. This is relevant because in most cases some words needs to have more weight than others in order to understand the polarization of the sentence. It has been shown that adding an attention model substantially improves the accuracy of aspect-level sentiment classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "66iJp2QIzk7M"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment-3.1.3.Discussion.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
